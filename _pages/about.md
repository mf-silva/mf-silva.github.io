---
permalink: /
title: "Geometry and Deep Learning"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Iâ€™m a PhD candidate in Computer Science at Dalhousie University. My research broadly focuses on the applications of (differential) geometry to deep learning; most recently studied how a Riemannian approach to transformer symmetries can recover the relation between sharpness and generalization in transformers, work accepted at ICML 2025  as a Spotlight Poster. I previously trained in theoretical and astrophysical physics, which might be where I got the notion to study geometry for deep learning.


Why geometry matters for deep learning
======

Problems once thought to be hopelessly high-dimensional (and hence afflicted with the curse of dimensionality) turn out to be tractable when we combine large-scale compute with just two algorithmic ideas: a) representation learning, where networks discover hierarchical features that capture the right regularities for each task; b) local optimization through gradient descent. The main question I am interested in is: why does this work at all?
My hope is that geometry offers an unifying language to understand this. Real-world tasks and architectures are structured, not generic. And this structure can be understood through geometry. Data concentrate on low-dimensional manifolds (the manifold hypothesis); parameters live in spaces with symmetries and constraints; optimization trajectory flow on curved surfaces; and generalization depends on the local and global shape of the loss. Getting a firmer grasp on these geometric structures is a path to models that are robust, sample-efficient, and performant.

**If this seems interesting to you, feel free to send me an e-mail; I am always looking for collaborators!**
